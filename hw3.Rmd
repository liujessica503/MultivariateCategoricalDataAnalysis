---
title: "STATS 601 HW3"
author: "Jessica Liu"
output: html_document
---

## Problem 3A
```{r}
# generate data
library(MASS)
set.seed(355)
mu = rep(0,7)
n = 100
lambda = t(rbind(c(rep(1,4),rep(0,3)),c(rep(0,3),rep(1,4))))
y = c() # store observations
x_list = c() # store x's
Psi = 0.4*diag(7)
currMu = matrix(rep(0,7)) # initialize sample mean
for(i in 1:n){ 
  # draw a 2x1 x
  x <- mvrnorm(n = 1, mu=c(0,0), Sigma=diag(2), empirical = FALSE)
  # draw a 7x1 w
  w <- mvrnorm(n = 1, mu=rep(0,7), Sigma=Psi, empirical = FALSE)
  # get a 7 x 100 vector, 100 observations of Y, Y is 7x1
  currY <- mu + lambda%*%x + w 
  currMu <- currMu + currY
  x_list <- cbind(x_list, x)
  y <- cbind(y,currY)
}

currMu <- (1/n)*currMu # sample mean

# EM Algorithm
library(Matrix)
# initialize
currPsi = 0.4*diag(7)+0.01
sigma = runif(1,0.01,0.2)
currLambda1 <- mvrnorm(1,mu = c(1,1,1,1,0,0,0),Sigma = sigma^2*diag(7))
currLambda2 <- mvrnorm(1,mu = c(0,0,0,1,1,1,1),Sigma = sigma^2*diag(7))
currLambda <- cbind(currLambda1,currLambda2)
W = currLambda
for(iter in 1:100){
  g = solve(diag(2)+t(W)%*%solve(currPsi)%*%W)
  # terms in updated lambda
  W1_sum = 0
  W2_sum = 0
  # terms in updated Psi
  Psi_sum = 0
  Y = c() # nxq, nth row is t(y_n - currMu)
  for(i in 1:n){
    Ez = g%*%t(W)%*%solve(currPsi)%*%(y[,i]-currMu)
    Ezzt = g + Ez%*%t(Ez)
    # calculate terms in W
    W_1 = (y[,i]-currMu)%*%t(Ez)
    W1_sum = W1_sum + W_1
    W2_sum = W2_sum + Ezzt
    Psi_1 = Ez%*%t(y[,i]-currMu)
    Psi_sum = Psi_1 + Psi_sum
    Y = rbind(Y, t(y[,i]-currMu))
  }
  Wnew = W1_sum%*%solve(W2_sum)
  S = 1/n*t(Y)%*%Y # 7x7
  PsiNew = diag(S - (Wnew/n)%*%Psi_sum)
  PsiNew = Matrix::Diagonal(7, x = PsiNew)
  currPsi = PsiNew
  currLambda = Wnew
  W = currLambda
}
currPsi
currLambda
currLambda_A <- currLambda # save for future use
# compare to factor analysis
library(stats)
factanal(t(y),factors=2)$loadings

```
We see that our maximized lambda generated by the EM algorithm is pretty close to the true lambda. The lambda is also close to the loadings from the factanal package.


## Problem 3B
```{r}
# initialize
currPsi = 0.4*diag(7)+0.01
# currLambda drawn without knowledge of population lambda
currLambda1 <- mvrnorm(1,mu = rep(-1,7),Sigma = diag(7))
currLambda2 <- mvrnorm(1,mu = rep(-2,7),Sigma = diag(7))
currLambda <- cbind(currLambda1,currLambda2)
for(iter in 1:200){
  W = currLambda
  g = solve(diag(2)+t(W)%*%solve(currPsi)%*%W)
  # terms in updated lambda
  W1_sum = 0
  W2_sum = 0
  # terms in updated Psi
  Psi_sum = 0
  Y = c() # nxq, nth row is t(y_n - currMu)
  for(i in 1:n){
    Ez = g%*%t(W)%*%solve(currPsi)%*%(y[,i]-currMu)
    Ezzt = g + Ez%*%t(Ez)
    # calculate terms in W
    W_1 = (y[,i]-currMu)%*%t(Ez)
    W1_sum = W1_sum + W_1
    W2_sum = W2_sum + Ezzt
    Psi_1 = Ez%*%t(y[,i]-currMu)
    Psi_sum = Psi_1 + Psi_sum
    Y = rbind(Y, t(y[,i]-currMu))
  }
  Wnew = W1_sum%*%solve(W2_sum)
  S = 1/n*t(Y)%*%Y # 7x7
  PsiNew = diag(S - (Wnew/n)%*%Psi_sum)
  PsiNew = Matrix::Diagonal(7, x = PsiNew)
  currPsi = PsiNew
  currLambda = Wnew
}
currPsi
currLambda
currLambda_B <- currLambda # save for future use


# calculate distance between column space of currLambda and population lambda
ColcurrLambda <- currLambda_B%*%solve(t(currLambda_B)%*%currLambda_B)%*%t(currLambda_B)
ColLambda <- lambda%*%solve(t(lambda)%*%lambda)%*%t(lambda)
norm(ColcurrLambda - ColLambda, type = 'F') # Frobenius norm of the distance

```
We can use the Frobenius norm to measure the distance from the projection of our lambda using the EM algorithm (currLambda_A) to the projection of our true lambda. The small norm shows us that the maximized lambda is pretty close to the true lambda.


## Problem 3C
```{r}
set.seed(5)
n = 100
sigma = runif(1,0.01,0.2)
currLambda1 <- mvrnorm(1,mu = c(1,1,1,1,0,0,0),Sigma = sigma^2*diag(7))
currLambda2 <- mvrnorm(1,mu = c(0,0,0,1,1,1,1),Sigma = sigma^2*diag(7))
currLambda <- cbind(currLambda1,currLambda2)
W = currLambda
currMu = currMu # from above, sample mean of y
sigmaSq = 0.4 # sigma squared, from W variance is sigma^2 * Identity
M = t(W)%*%W+sigmaSq*diag(2)


for(iter in 1:200){
W1_sum = 0
W2_sum = 0
sigmaSq_sum = 0
for(i in 1:n){
  Ez = solve(M)%*%t(W)%*%(y[,i]-currMu)
  Ezzt = sigmaSq*solve(M)+Ez%*%t(Ez)
  # calculate terms in W
  W_1 = (y[,i]-currMu)%*%t(Ez)
  W1_sum = W1_sum + W_1
  W2_sum = W2_sum + Ezzt
  sigmaSq_1 = norm(y[,i]-currMu, type="2")^2-2*t(Ez)%*%t(W)%*%(y[,i]-currMu)+pracma::Trace(Ezzt%*%t(W)%*%W)
  sigmaSq_sum = sigmaSq_sum + sigmaSq_1
}
Wnew = W1_sum%*%solve(W2_sum)
# d = 7
new_sigmaSq = 1/(7*n)*sigmaSq_sum
W = Wnew
sigmaSq = new_sigmaSq[1] # change from 1x1 matrix to a scalar
}
W
sigmaSq

# compare with standard pca and results from part a, part b
library(pracma)
library(psych)
W <- orth(W)
ty <- t(y) # transpose data
evs = eigen(cov(ty %*% W))
evecs = evs$vectors
loadings_C = W %*% evecs
scores_C = ty %*% W
Xrecon_C = scores_C %*% t(W) # reconstruct data
reconerr_C = sum((Xrecon_C-ty)^2) 

# results from part a
W <- orth(as.matrix(currLambda_A))
evs = eigen(cov(ty %*% W))
evecs = evs$vectors
loadings_A = W %*% evecs
scores_A = ty %*% W
Xrecon_A = scores_A %*% t(W) # reconstruct data
reconerr_A = sum((Xrecon_A-ty)^2) 

# results from part b
W <- orth(as.matrix(currLambda_B))
evs = eigen(cov(ty %*% W))
evecs = evs$vectors
loadings_B = W %*% evecs
scores_B = ty %*% W
Xrecon_B = scores_B %*% t(W) # reconstruct data
reconerr_B = sum((Xrecon_B-ty)^2) 

### compare to standard pca on full data set, first two principal components
origpca =  princomp(scale(ty))
loadings_origpca = origpca$loadings[,1:2]
# scores (loadings) may have opposite signs for origpca and EM
# when using probabilistic PCA or princomp, you'll notice W and Z might have different signs between PCA and probabilistic PCA, but the absolute value will be the same.
loadings_origpca - loadings_A
loadings_origpca - loadings_B
loadings_origpca - loadings_C
```
We see that after 200 iterations of the EM algorithm for probabilistic PCA, the maximized W is close to the true lambda, and the maximized sigma is close to the true sigma.
The loadings from EM algorithm are reasonably close to the PCA loadings, except for the results in part (b) are worse.


## Problem 4
```{r}
airlineData <- matrix(NA,10,10)
airlineData[lower.tri(airlineData,diag=TRUE)] <- c(0,587,1212,701,1936,604,748,2139,2182,543,0,920,940,1745,1188,713,1858,1737,597,0,879,831,1726,1631,949,1021,1494,0,1374,968,1420,1645,1891,1220,0,2339,2451,347,959,2300,0,1092,2594,2734,923,0,2571,2408,205,0,678,2442,0,2329)
names <- c("ATL","CHI","DEN","HOUS","LA","MIAMI","NY","SF","SEAT","WASH")
row.names(airlineData) <- names
colnames(airlineData) <- names

# Classical MDS
# N rows (objects) x p columns (variables)
# each row identified by a unique row name

d <- dist(airlineData) # euclidean distances between the rows
fit <- cmdscale(d,eig=TRUE, k=2) # k is the number of dim
fit # view results

# plot solution 
x <- fit$points[,1]
y <- fit$points[,2]
plot(x, y, xlab="Coordinate 1", ylab="Coordinate 2", 
     main="Multidimensional Scaling", pch = 19, type="n")
text(x, y, labels = row.names(airlineData), cex=.7)
```
